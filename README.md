# CDSC

During the process of training machine learning models, selecting the right stopping criterion is crucial to prevent overfitting and conserve computing power. While the early stopping and the maximum number of epochs stopping methods are simple to implement, they have limitations in identifying the point during training where the training and validation loss start to diverge. To overcome these limitations, we propose a general correlation-based stopping criterion and compare it with the early stopping and maximum number of epochs stopping methods across multiple common machine learning problems and models. Our study shows that the newly proposed Correlation-Driven Stopping Criterion (CDSC) can enhance the out-of-sample performance of all tested machine learning models while conserving computing power.
