# Correlation-Driven Stopping Criterion

## A novel method to stop the learning process of neural networks timely and prevent the overfitting of the resulting model

During the process of training machine learning models, selecting the right stopping criterion is crucial to prevent overfitting and conserve computing power. While the early stopping and the maximum number of epochs stopping methods are simple to implement, they have limitations in identifying the point during training where the training and validation loss start to diverge. To overcome these limitations, we propose a general correlation-based stopping criterion and compare it with the early stopping and maximum number of epochs stopping methods across multiple common machine learning problems and models. Our study shows that the newly proposed Correlation-Driven Stopping Criterion (CDSC) can enhance the out-of-sample performance of all tested machine learning models while conserving computing power.

The article presenting the CDSC method was is available at: [https://doi.org/10.1016/j.neucom.2023.127028](https://doi.org/10.1016/j.neucom.2023.127028)

Cite this article:
Miseta T., Fodor A., Vathy-Fogarassy √Å. (2024). Surpassing early stopping: A novel correlation-based stopping criterion for neural networks. _Neurocomputing_, 567, 127028 

Codes are coming soon... (in a few weeks) :)

