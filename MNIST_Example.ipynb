{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1fe06d1",
   "metadata": {},
   "source": [
    "# Surpassing Early Stopping: </br> A Correlation-driven stopping criterion for machine learning models.\n",
    "\n",
    "### *A hands-on guide with the MNIST Handwritten Digit Classification Problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e930b",
   "metadata": {},
   "source": [
    "Machine learning models are easy to overfit and hard to regularize. Usually, there is not enough emphasis on the out-of-sample performance of these models, which is why they underperform. There are numerous methods to regularize machine learning models, however, the best way is still stopping the training at the right time, before overfitting occurs. The vast majority of machine learning models simply stop the training at a pre-defined epoch number or utilize early stopping. Is there a better way to do it? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357b80e",
   "metadata": {},
   "source": [
    "## The Correlation-driven stopping criterion (CDSC)\n",
    "\n",
    "The Correlation-Driven Stopping Criterion (CDSC) is designed to address limitations in existing stopping strategies like early stopping and maximum epoch-based methods. CDSC works by monitoring the rolling Pearson correlation between the training and the validation loss metrics. Training is stopped when this correlation drops below a predefined threshold and stays below a pre-defined number of epochs. This approach helps in accurately determining the optimal point to stop training, thus preventing overfitting and improving the model's generalization to data not seen before. It's a flexible method that can be fine-tuned for different scenarios, offering a significant improvement in efficiency and performance compared to traditional methods. </br> Let's see how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdd7758",
   "metadata": {},
   "source": [
    "![The method](./method.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d7455",
   "metadata": {},
   "source": [
    "The basis of the method is the rolling Pearson correlation coefficient between the training and the test loss. The window in which the rolling correlation is calculated is shifted as the training progresses, it contains the metrics of the last ω epochs. Before epoch ω no correlation is calculated. If the first Pearson rolling correlation at epoch ω is negative or really low, the model or the problem is likely to be malformulated.\n",
    "We introduce a threshold value μ. If the correlation falls below the patience value, we increment a counter. In the figure, the epoch where the correlation falls below the threshold is denoted with a vertical blue dashed line.\n",
    "We introduce a patience value λ. We stop the training if the counter reaches  λ. This is the epoch where the training stops. In the figure, this epoch is denoted with a vertical red dashed line. Finally, we chose the model with the best validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac8fa8",
   "metadata": {},
   "source": [
    "# Clone the repo and install the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/vathyfogarassy/CDSC\n",
    "!pip install tensorflow\n",
    "!pip install matplotlib\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f3ca4",
   "metadata": {},
   "source": [
    "# About the dataset\n",
    "\n",
    "The MNIST dataset is a large database of handwritten digits that is widely used for training and testing in the field of machine learning. MNIST stands for \"Modified National Institute of Standards and Technology.\" The dataset contains 70,000 images of handwritten digits, from 0 to 9, which are divided into a training set of 60,000 examples and a test set of 10,000 examples.\n",
    "\n",
    "Each image in the MNIST dataset is a 28x28 pixel grayscale representation of a digit. These images have been size-normalized and centered in a fixed-size image. The simplicity of the MNIST dataset makes it a standard benchmark for evaluating the performance of a wide range of machine learning algorithms, especially those involving image recognition and computer vision.\n",
    "\n",
    "We are going to divide the dataset into training, validation and test datasets, to benchmark the CDSC method and to see how it operates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846214ab",
   "metadata": {},
   "source": [
    "# Dataset and data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d4c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "(train_data, train_targets), (test_data, test_targets) = mnist.load_data()\n",
    "randomizer = np.arange(len(test_data))\n",
    "np.random.shuffle(randomizer)\n",
    "test_data = test_data[randomizer]\n",
    "test_targets = test_targets[randomizer]\n",
    "\n",
    "print(train_data.shape)\n",
    "valid_data = test_data[:-1:2,:,:]\n",
    "valid_targets = test_targets[:-1:2]\n",
    "test_data = test_data[1::2,:,:]\n",
    "test_targets = test_targets[1::2]\n",
    "train_data = np.expand_dims(train_data.astype(\"float32\")/255,-1)\n",
    "valid_data = np.expand_dims(valid_data.astype(\"float32\")/255,-1)\n",
    "test_data = np.expand_dims(test_data.astype(\"float32\")/255,-1)\n",
    "\n",
    "\n",
    "\n",
    "train_targets = tf.keras.utils.to_categorical(train_targets, num_classes)\n",
    "valid_targets = tf.keras.utils.to_categorical(valid_targets, num_classes)\n",
    "test_targets = tf.keras.utils.to_categorical(test_targets, num_classes)\n",
    "    \n",
    "dataset_save_path = \"\"\n",
    "np.save(dataset_save_path+\"TrainX.npy\",train_data)\n",
    "np.save(dataset_save_path+\"ValidX.npy\",valid_data)\n",
    "np.save(dataset_save_path+\"TestX.npy\",test_data)\n",
    "np.save(dataset_save_path+\"TrainY.npy\",train_targets)\n",
    "np.save(dataset_save_path+\"ValidY.npy\",valid_targets)\n",
    "np.save(dataset_save_path+\"TestY.npy\",test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3375a0b",
   "metadata": {},
   "source": [
    "The code above creates 6 datsets, targets and input data for the training, validation and test sets. We save these for further usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dfed65",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "Second, we need to define the model to solve the handwritten digit classification problem. We will be using a CNN-based model. Despite its computational cost it is still commonly used for image processing tasks. We import the CDSC stopping method, which is implemented as a TensorFlow callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from CDSC_callback import CDSC\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "    \n",
    "filepath = \"Best{val_loss:.2f}.hdf5\"\n",
    "    \n",
    "CBCB = CDSC(filepath = filepath,\n",
    "                    window_size = 5,\n",
    "                    threshold = 0.4,\n",
    "                    patience = 10,\n",
    "                    )\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1beece",
   "metadata": {},
   "source": [
    "We compile the model and observe the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa1199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6b145e",
   "metadata": {},
   "source": [
    "We train the model with the CDSC method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1baa4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CDSC_callback import CDSC\n",
    "\n",
    "model.fit(train_data,train_targets,\n",
    "              validation_data = (valid_data,valid_targets),\n",
    "              epochs=150,\n",
    "              shuffle=True,\n",
    "              batch_size = 256,\n",
    "              callbacks = [CBCB,tf.keras.callbacks.CSVLogger(\"LOG.csv\", separator=',', append=False)],\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee370a38",
   "metadata": {},
   "source": [
    "During training, the callback prints the training and the validation loss along with the correlation between them. If the problem is formulated correctly, after epoch omega, the correlation starts high and decreases during the training, until the stopping criterion triggers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c49e5",
   "metadata": {},
   "source": [
    "# Results with cross-validation\n",
    "\n",
    "We tested the CDSC method with several different datasets and models. We used a cross-validation of 50 and conducted an extensive hyperparameter search on all three parameters.\n",
    "We summarized our testing methodology in a figue:\n",
    "\n",
    "![The method](./Testmethod.jpeg)\n",
    "\n",
    "The results can be observed in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18dd206",
   "metadata": {},
   "source": [
    "| Dataset                       | $\\mu_{Best}$ | $\\omega_{Best}$ | $\\lambda_{Best}$ | $\\overline{n}_{Stop}^{CD(pset_{Best})}$ | $\\overline{n}_{Best}^{CD(pset_{Best})}$ | $\\overline{\\%e}_{ts}^{CD(pset_{Best})}$ |\n",
    "|-------------------------------|--------------|-----------------|------------------|------------------------------------------|------------------------------------------|------------------------------------------|\n",
    "| Credit Card Fraud Detection   | 0.75         | 30              | 65               | 119.28                                   | 79.00                                    | -1.35                                    |\n",
    "| MNIST                         | 0.40         | 35              | 10               | 76.30                                    | 61.26                                    | -2.89                                    |\n",
    "| Boston Hs.                    | -0.40        | 10              | 25               | 197.76                                   | 108.28                                   | -1.56                                    |\n",
    "| Gold Daily Price Change       | -0.15        | 30              | 25               | 99.54                                    | 39.22                                    | -0.27                                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ce71d",
   "metadata": {},
   "source": [
    "Where $\\overline{n}_{Stop}^{CD(pset_{Best})}$ is the epoch where the training was stopped, $\\overline{n}_{Best}^{CD(pset_{Best})}$ is the epoch where the validation error was the lowest, this model was selected for further usage, $\\overline{\\%e}_{ts}^{CD(pset_{Best})}$ is the percentage test error reduction from the baseline we used for the testing, which was an epoch limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893f5530",
   "metadata": {},
   "source": [
    "But how does it compare to the early stopping and the epoch limit methods with optimal parameters? Let's put these results in perspective?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b06ff",
   "metadata": {},
   "source": [
    "| Dataset                  | $\\overline{\\%e}_{ts}^{ME(m_{Best})}$ | $\\overline{\\%e}_{ts}^{ES(p_{Best})}$ | $\\overline{\\%e}_{ts}^{CD(pset_{Best})}$ |\n",
    "|--------------------------|--------------------------------------|--------------------------------------|------------------------------------------|\n",
    "| Credit Card Fraud Detection | -1.20                               | -1.25                                | -1.35                                   |\n",
    "| MNIST                    | -2.72                                | -0.95                                | -2.89                                   |\n",
    "| Boston Housing           | -0.18                                | 0.00                                 | -1.56                                   |\n",
    "| Gold Daily Price Change  | -0.05                                | -0.15                                | -0.27                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b61e6eb",
   "metadata": {},
   "source": [
    "where $\\overline{\\%e}_{ts}^{ME(m_{Best})}$, $\\overline{\\%e}_{ts}^{ES(p_{Best})}$, and $\\overline{\\%e}_{ts}^{CD(pset_{Best})}$ are the percentage error reductions from the baseline for the maximum number of epochs, the early stopping and the CDSC methods respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec0826",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, we learned how to use the Correlation-driven stopping criterion with the MNIST dataset and we also got a grasp of the potential of the algorithm. The new method is capable of achieving better results than the early stopping and the maximum number of epochs methods.\n",
    "\n",
    "All results presented in this article are based on the following publication:\n",
    "\n",
    "Miseta, Tamás, Attila Fodor, and Ágnes Vathy-Fogarassy. \"Surpassing early stopping: A novel correlation-based stopping criterion for neural networks.\" Neurocomputing 567 (2024): 127028.\n",
    "\n",
    "If it was useful for your research work, please consider citing the article\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
